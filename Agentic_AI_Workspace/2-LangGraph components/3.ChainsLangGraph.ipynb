{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc666a7",
   "metadata": {},
   "source": [
    "# Chain using LangGraph\n",
    "\n",
    "LangGraph uses 4 important concepts\n",
    "\n",
    "- How to use chat messages as our graph state\n",
    "- How to use chat models in graph nodes\n",
    "- How to bind tools to our LLM in chat models\n",
    "- How to execute the tools call in out graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84e31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd323c9",
   "metadata": {},
   "source": [
    "# How to use chat messages a out graph state\n",
    "\n",
    "Messages\n",
    "\n",
    "We can use messages which can be used to capture different roles with in a conversation. LangChain has various message typed incliding HumanMessage, AIMessage, SystemMessage and ToolMessage. These represent a message from the user, from chat model, \n",
    "\n",
    "Every message have these important components.\n",
    "\n",
    "- content- content of the message\n",
    "- name- Specify the name of author\n",
    "- response_metadata - Optionally, a dict of metadata (eg., often populated by model provider for AIMessages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "718f3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: LLM-Model\n",
      "\n",
      "Please tellme how can I help\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: BoonSai\n",
      "\n",
      "I want to learn coding!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: LLM-Model\n",
      "\n",
      "Which programming languge you want to learn\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: BoonSai\n",
      "\n",
      "Hey! I want to learn python programming langugae\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from pprint import pprint # this lib is to print in beautiful manner (pretty print)\n",
    "\n",
    "messages = [AIMessage(content=f\"Please tellme how can I help\", name=\"LLM-Model\")]\n",
    "messages.append(HumanMessage(content=f\"I want to learn coding!\", name=\"BoonSai\"))\n",
    "messages.append(AIMessage(content=f\"Which programming languge you want to learn\", name=\"LLM-Model\"))\n",
    "messages.append(HumanMessage(content=f\"Hey! I want to learn python programming langugae\", name=\"BoonSai\"))\n",
    "\n",
    "for message in messages: message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a91304",
   "metadata": {},
   "source": [
    "# Chat models\n",
    "\n",
    "We can use the sequence of messages a input with the chat-models using LLM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e785b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model = 'llama-3.1-8b-instant')\n",
    "result = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a2fc0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 491,\n",
       "  'prompt_tokens': 82,\n",
       "  'total_tokens': 573,\n",
       "  'completion_time': 0.688799497,\n",
       "  'prompt_time': 0.004415927,\n",
       "  'queue_time': 0.044415641,\n",
       "  'total_time': 0.693215424},\n",
       " 'model_name': 'llama-3.1-8b-instant',\n",
       " 'system_fingerprint': 'fp_a7a2f9abbf',\n",
       " 'service_tier': 'on_demand',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata # it prints metadata of the llm's output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b106da9",
   "metadata": {},
   "source": [
    "# Tools\n",
    "\n",
    "-Tools can be integrated with the LLM models to interact with external systems. External systems can be API's, third party tools. \n",
    "\n",
    "- Whenever a query is asked the model can choose to call the tool and this query is based on the natural language input and this will return and output that matches the tool's schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb9151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a: int, b:int)-> int:\n",
    "    \"\"\" Add a and b\n",
    "    Args:\n",
    "        a (int): First int\n",
    "        b (int): second int\n",
    "    \n",
    "    returns:\n",
    "        int     \n",
    "    \"\"\"\n",
    "    return a+b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32cabb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x124a01d10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x124a75c90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad64b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'wxbvaw164', 'function': {'arguments': '{\"a\":2,\"b\":2}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 269, 'total_tokens': 287, 'completion_time': 0.038753537, 'prompt_time': 0.042570129, 'queue_time': 0.04549145, 'total_time': 0.081323666}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_3ddc9808b3', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--c26f1654-1262-4fd8-b9f8-ad2fe5737894-0', tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 2}, 'id': 'wxbvaw164', 'type': 'tool_call'}], usage_metadata={'input_tokens': 269, 'output_tokens': 18, 'total_tokens': 287})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Binding tool with llm\n",
    "\n",
    "llm_with_tools = llm.bind_tools([add])\n",
    "\n",
    "llm_with_tools.invoke([HumanMessage(content=f\"what is 2 plus 2\", name='BoonSai')]) # here it shows empty message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae04c09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731c831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ec033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d004d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
